{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from dask.distributed import Client, progress\n",
    "import dask.bag as db\n",
    "import math\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.matutils import sparse2full\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(row):\n",
    "    cat = row.category if 'category' in row else None\n",
    "    \n",
    "    if cat is None or pd.isna(cat) and 'type' in row and not pd.isna(row.type):\n",
    "        return row.type\n",
    "    elif isinstance(cat,dict) and 'key' in cat and not pd.isna(cat['key']):\n",
    "        return cat['key']\n",
    "    elif 'ranking_category' in row and not pd.isna(row.ranking_category):\n",
    "        return row.ranking_category\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('/Users/danmer/travel_data/source/bc_clean.json')\n",
    "van_df = pd.read_json('/Users/danmer/travel_data/source/vancouver_clean.json')\n",
    "data_df = pd.concat([data_df, van_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.category = data_df.apply(extract_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractions = data_df[(data_df.category == 'attraction') & (data_df.rating.notnull())][['id', 'name','rating','reviews', 'website']]\n",
    "attractions['num_reviews'] = attractions.apply(lambda r: len(r.reviews) if 'reviews' in r else 0, axis=1)\n",
    "attractions.sort_values('num_reviews',ascending=False).drop_duplicates('name').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractions[attractions.rating == 5].sort_values('num_reviews',ascending=False).drop_duplicates('name').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = data_df[(data_df.category == 'RESTAURANT') & (data_df.rating.notnull())][['name','rating','reviews']]\n",
    "restaurants['num_reviews'] = restaurants.apply(lambda r: len(r.reviews) if 'reviews' in r else 0, axis=1)\n",
    "restaurants.sort_values('num_reviews',ascending=False).drop_duplicates('name').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(data_df[(data_df.category == 'attraction') & (data_df.description.notnull()) \\\n",
    "                            & (data_df.description!='')].description)\n",
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove + TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reviews aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_agg_reviews = {}\n",
    "skipped = 0\n",
    "att_df = data_df[(data_df.category == 'attraction')].drop_duplicates('name')[['name', 'reviews']]\n",
    "for idx, (id, row) in enumerate(att_df.iterrows()):\n",
    "    name, reviews = row\n",
    "    res_reviews = []\n",
    "\n",
    "    for rev in reviews:\n",
    "        try:\n",
    "            if rev['language'] == 'en':\n",
    "                res_reviews.append(rev['text'])\n",
    "        except Exception as ex:\n",
    "            skipped += 1\n",
    "            pass\n",
    "    att_agg_reviews[name] = '\\n'.join(res_reviews)\n",
    "print(f'skipped: {skipped}')\n",
    "len(att_agg_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/danmer/travel_data/tags/docs.json', 'w+') as f:\n",
    "    json.dump(att_agg_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(att_agg_reviews.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(att_agg_reviews.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1999 [2:05:56<260:09:18, 472.29s/it]\n"
     ]
    }
   ],
   "source": [
    "nlp_model  = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_doc(text, nlp):\n",
    "    def keep_token(t):\n",
    "        return (t.is_alpha and \n",
    "                not (t.is_space or t.is_punct or \n",
    "                     t.is_stop or t.like_num))\n",
    "\n",
    "    def lemmatize_doc(doc):\n",
    "        return [ t.lemma_ for t in doc if keep_token(t)]\n",
    "    \n",
    "    def ent_doc(doc):\n",
    "        return [(e.text, e.label_) for e in doc.ents]\n",
    "    \n",
    "    nlp_doc = nlp(text.lower())\n",
    "    \n",
    "    return {'tokens': lemmatize_doc(nlp_doc), \n",
    "            'entities': ent_doc(nlp_doc),\n",
    "           'emb': nlp_doc.vector}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(att_agg_reviews.values())\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "# http://localhost:8787/status http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_docs(texts):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return [nlp_doc(text, nlp) for text in texts]\n",
    "\n",
    "nlp_docs = db.from_sequence(docs).repartition(math.ceil(len(docs)/100)).map_partitions(nlp_docs).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nlp_doc in nlp_docs:\n",
    "    nlp_doc['emb'] = nlp_doc['emb'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/danmer/travel_data/tags/nlp_docs.json', 'w+') as f:\n",
    "    json.dump({name: d for d, name in zip(nlp_docs, att_agg_reviews.keys())}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(docs):\n",
    "    docs_dict = Dictionary(docs)\n",
    "    docs_dict.filter_extremes(no_below=20, no_above=0.2)\n",
    "    docs_dict.compactify()\n",
    "\n",
    "    docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    return docs_corpus, docs_dict\n",
    "\n",
    "def build_tfidf(docs_corpus, docs_dict):\n",
    "    model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "    return model_tfidf\n",
    "\n",
    "\n",
    "def save_tfidf(model_tfidf, path):\n",
    "    model_tfidf.save(path)\n",
    "\n",
    "\n",
    "def load_tfidf(path):\n",
    "    return TfidfModel.load(path)\n",
    "\n",
    "\n",
    "def corpus_from_nlp_docs(nlp_docs):\n",
    "    return get_corpus([d['tokens'] for d in nlp_docs])\n",
    "\n",
    "\n",
    "def get_sorted_tfidf_with_labels(docs_dict, doc_tfidf):\n",
    "    return sorted([{'tag': docs_dict[id], 'weight':  w} \n",
    "                   for id, w in doc_tfidf], key=lambda x: x['weight'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_corpus, docs_dict = corpus_from_nlp_docs(nlp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2929"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = build_tfidf(docs_corpus, docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tfidf(tfidf, '/Users/danmer/travel_data/tags/tfidf.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_tfidf = tfidf[docs_corpus]\n",
    "tags_dict = {name: get_sorted_tfidf_with_labels(docs_dict, doc_tfidf) \n",
    "              for name, doc_tfidf in zip(names, transformed_tfidf)}\n",
    "len(tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tag': 'beach', 'weight': 0.7770620885352576},\n",
       " {'tag': 'shore', 'weight': 0.18827467972091194},\n",
       " {'tag': 'sunset', 'weight': 0.16856450115555727},\n",
       " {'tag': 'north', 'weight': 0.13881659659598547},\n",
       " {'tag': 'dark', 'weight': 0.1348516009244458},\n",
       " {'tag': 'bank', 'weight': 0.1220805388643879},\n",
       " {'tag': 'ocean', 'weight': 0.11948429338482722},\n",
       " {'tag': 'sandy', 'weight': 0.11785924677357029},\n",
       " {'tag': 'bay', 'weight': 0.11282639167940087},\n",
       " {'tag': 'swimming', 'weight': 0.09992094973061863}]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_dict['Jericho Beach'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hybrid embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embs(model_tfidf, nlp, nlp_docs):\n",
    "    # http://dsgeek.com/2018/02/19/tfidf_vectors.html\n",
    "    docs_corpus, docs_dict = corpus_from_nlp_docs(nlp_docs)\n",
    "    docs_tfidf  = model_tfidf[docs_corpus]\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "\n",
    "#     tfidf_emb_vecs = np.vstack([np.array(nlp_docs[i]['emb']) for i in range(len(nlp_docs))])\n",
    "    tfidf_emb_vecs = np.vstack([nlp(docs_dict[i]).vector for i in range(len(docs_dict))])\n",
    "    docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "    \n",
    "    return docs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_reviews_embs = get_embs(tfidf, nlp_model, nlp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dict = {k:v for k,v in zip(names, agg_reviews_embs.tolist())}\n",
    "len((emb_dict['Jericho Beach']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('attractions-agg-reviews-embs.txt', agg_reviews_embs, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attractions-agg.txt\", \"w\") as outf:\n",
    "    outf.write('\\n'.join([k.replace('\\n',' ') for k in att_agg_reviews.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lovely walk with great views to sea and lots of waterfowl on the water.\\nWe went to Neck Point Park to just put our feet in the water and we end up taking many pictures, stopping to appreciate the views, and walking one of the many trails offered.\\nWhat a great place to walk the trails and see the beauty of the ocean.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  gensim.summarization\n",
    "\n",
    "def summarize(text):\n",
    "    return gensim.summarization.summarize(text,  word_count=50)\n",
    "\n",
    "summarize(docs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = math.ceil(len(texts)/100)\n",
    "summaries = db.from_sequence(texts).repartition(partitions).map_partitions(summarize).compute()\n",
    "\n",
    "summaries_didct = {name: summary for name, reviews in zip(names, summaries)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = data_df[(data_df.category == 'attraction'].drop(columns=['reviews']).drop_duplicates('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: filter tours and advernurej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = np.ones((300,), dtype='float32').tolist()\n",
    "export_df['embedding'] = export_df.apply(lambda x: emb_dict[x['name']] if x['name'] in emb_dict and all(e != 0 for e in emb_dict[x['name']]) else default, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert export_df[export_df.name == 'Jericho Beach'].embedding.values[0] == emb_dict['Jericho Beach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(row):\n",
    "    if \"longitude\" not in row or 'latitude' not in row or pd.isna(row.latitude) or pd.isna(row.longitude):\n",
    "        return None\n",
    "    return f'{row.latitude},{row.longitude}'\n",
    "export_df['location'] = export_df.apply(get_location, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert export_df[export_df.name == 'Jericho Beach'].location.values[0] == '49.273098,-123.20285'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df['tags'] = export_df.apply(lambda x: tags_dict[x['name']] if x['name'] in tags_dict and tags_dict[x['name']]  else {}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1999 entries, 16752 to 6208\n",
      "Data columns (total 74 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   id                                         0 non-null      float64\n",
      " 1   type                                       0 non-null      object \n",
      " 2   name                                       1999 non-null   object \n",
      " 3   awards                                     1999 non-null   object \n",
      " 4   rankingPosition                            0 non-null      float64\n",
      " 5   priceLevel                                 0 non-null      object \n",
      " 6   category                                   1999 non-null   object \n",
      " 7   rating                                     1670 non-null   float64\n",
      " 8   hotelClass                                 0 non-null      float64\n",
      " 9   phone                                      1570 non-null   object \n",
      " 10  address                                    1999 non-null   object \n",
      " 11  email                                      1021 non-null   object \n",
      " 12  amenities                                  0 non-null      object \n",
      " 13  prices                                     0 non-null      object \n",
      " 14  latitude                                   1860 non-null   float64\n",
      " 15  longitude                                  1860 non-null   float64\n",
      " 16  webUrl                                     0 non-null      object \n",
      " 17  website                                    1752 non-null   object \n",
      " 18  rankingString                              0 non-null      object \n",
      " 19  numberOfReviews                            0 non-null      float64\n",
      " 20  rankingDenominator                         0 non-null      float64\n",
      " 21  hotelClassAttribution                      0 non-null      object \n",
      " 22  isClosed                                   0 non-null      float64\n",
      " 23  isLongClosed                               0 non-null      float64\n",
      " 24  cuisine                                    0 non-null      object \n",
      " 25  hours                                      791 non-null    object \n",
      " 26  location_id                                1999 non-null   float64\n",
      " 27  num_reviews                                1999 non-null   float64\n",
      " 28  timezone                                   1999 non-null   object \n",
      " 29  location_string                            1999 non-null   object \n",
      " 30  photo                                      1642 non-null   object \n",
      " 31  api_detail_url                             1999 non-null   object \n",
      " 32  location_subtype                           1999 non-null   object \n",
      " 33  doubleclick_zone                           1999 non-null   object \n",
      " 34  preferred_map_engine                       1999 non-null   object \n",
      " 35  raw_ranking                                1677 non-null   float64\n",
      " 36  ranking_geo                                1677 non-null   object \n",
      " 37  ranking_geo_id                             1677 non-null   float64\n",
      " 38  ranking_position                           1677 non-null   float64\n",
      " 39  ranking_denominator                        1677 non-null   float64\n",
      " 40  ranking_category                           1677 non-null   object \n",
      " 41  ranking_subcategory                        1677 non-null   object \n",
      " 42  subcategory_ranking                        1677 non-null   object \n",
      " 43  ranking                                    1677 non-null   object \n",
      " 44  distance                                   0 non-null      float64\n",
      " 45  distance_string                            0 non-null      float64\n",
      " 46  bearing                                    0 non-null      float64\n",
      " 47  is_closed                                  1999 non-null   float64\n",
      " 48  is_long_closed                             1999 non-null   float64\n",
      " 49  description                                1999 non-null   object \n",
      " 50  web_url                                    1999 non-null   object \n",
      " 51  write_review                               1999 non-null   object \n",
      " 52  ancestors                                  1999 non-null   object \n",
      " 53  subcategory                                1999 non-null   object \n",
      " 54  parent_display_name                        1999 non-null   object \n",
      " 55  is_jfy_enabled                             1999 non-null   float64\n",
      " 56  nearest_metro_station                      1999 non-null   object \n",
      " 57  address_obj                                1999 non-null   object \n",
      " 58  is_candidate_for_contact_info_suppression  1999 non-null   float64\n",
      " 59  subtype                                    1999 non-null   object \n",
      " 60  tags                                       1999 non-null   object \n",
      " 61  shopping_type                              1999 non-null   object \n",
      " 62  open_now_text                              791 non-null    object \n",
      " 63  booking                                    400 non-null    object \n",
      " 64  offer_group                                400 non-null    object \n",
      " 65  animal_welfare_tag                         123 non-null    object \n",
      " 66  fee                                        41 non-null     object \n",
      " 67  neighborhood_info                          1026 non-null   object \n",
      " 68  waypoint_info                              3 non-null      object \n",
      " 69  ta_message                                 1 non-null      object \n",
      " 70  machine_translated                         81 non-null     float64\n",
      " 71  translation_vendor                         6 non-null      object \n",
      " 72  embedding                                  1999 non-null   object \n",
      " 73  location                                   1999 non-null   object \n",
      "dtypes: float64(24), object(50)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "export_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df.to_json('/Users/danmer/travel_data/tags/elastic-bc-van-emb.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "  \"mappings\": {\n",
    "    \"dynamic_templates\": [\n",
    "      {\n",
    "        \"embs\": {\n",
    "          \"match\":   \"embedding\",\n",
    "          \"mapping\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 300\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"geo\": {\n",
    "          \"match\":   \"location\",\n",
    "          \"mapping\": {\n",
    "            \"type\": \"geo_point\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "    {\n",
    "        \"tags\": {\n",
    "          \"match\": \"tags\",\n",
    "          \"mapping\": {\n",
    "            \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"tag\": {\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    \"weight\": {\n",
    "                        \"type\": \"float\"\n",
    "                    }\n",
    "                }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import IndicesClient\n",
    "\n",
    "es = Elasticsearch(timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "index='ta-embs-tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ta-embs-tags'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index, body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = export_df.to_dict('records')\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took: 5704, errors: False\n"
     ]
    }
   ],
   "source": [
    "import simplejson\n",
    "lines = []\n",
    "for idx, doc in enumerate(records):\n",
    "    lines.append(json.dumps({ \"index\" : { \"_index\" : index, \"_id\" : f\"{idx}\"}}))\n",
    "    lines.append(simplejson.dumps(doc, ignore_nan=True))\n",
    "body = '\\n'.join(lines)\n",
    "res = es.bulk(index=index, body=body, timeout='10m')\n",
    "print(f'took: {res[\"took\"]}, errors: {res[\"errors\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in res['items'] if item['index']['status'] != 201 and item['index']['status'] != 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "def ner(texts):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs = [nlp(desc) for desc in texts]\n",
    "    return [[(e.text, e.label_) for e in doc.ents] for doc in docs ]\n",
    "partitions = math.ceil(len(reviews)/5000)\n",
    "with ProgressBar():\n",
    "    rev_db = db.from_sequence(reviews).repartition(partitions).map_partitions(ner).flatten().compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.json', 'w+') as f:\n",
    "    json.dump(rev_db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([en for en, label in rev_db if label == 'GPE']).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_docs = [nlp(desc) for desc in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in rev_docs[:5]:\n",
    "    displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in desc_docs[:5]:\n",
    "    displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ents = [e.text for doc in rev_docs for e in doc.ents if e.label_ == \"LOC\"]\n",
    "Counter(all_ents).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[(data_df.category == 'attraction')].sort_values('num_reviews',ascending=False).name.apply(lambda x: x.strip().lower()).drop_duplicates().to_csv('attractions', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser = Laser()\n",
    "embs = laser.embed_sentences(reviews[:1000], lang='en')\n",
    "# https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('review-embs', embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('review-embs.txt', embs, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reviews-10000.txt\", \"w\") as outf:\n",
    "    outf.write('\\n'.join([r.replace('\\n',' ')[:200] for r in reviews[:10000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model('/Users/danmer/deep-pdf-data/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_embs = []\n",
    "for review in tqdm(reviews[:10000]):\n",
    "    doc = nlp(review)\n",
    "    embs = []\n",
    "    for token in doc:\n",
    "        if token.text in model.words:\n",
    "            embs.append(model[token.text])\n",
    "    mean_emb = np.mean(np.array(embs), axis=0)\n",
    "    review_embs.append(mean_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('review-embs-fasttext-10000.txt', np.array(review_embs), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('review-embs-fasttext-10000.npy', np.array(review_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reviews-26k.txt\", \"w\") as outf:\n",
    "    outf.write('\\n'.join([r.replace('\\n',' ')[:200] for r in all_reviews]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=transformed_tfidf,\n",
    "                                           id2word=docs_dict,\n",
    "                                           num_topics=200, \n",
    "#                                            random_state=2,\n",
    "#                                            update_every=1,\n",
    "#                                            passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = pyLDAvis.gensim.prepare(lda_model, docs_corpus, docs_dict)\n",
    "dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=docs_corpus,\n",
    "                                                  texts=list(att_agg_reviews.keys()))\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "# Show\n",
    "df_dominant_topic.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "destinator",
   "language": "python",
   "name": "destinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
